\documentclass[twoside]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[ngerman,english]{babel}
\usepackage{fancyhdr,lastpage}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{longtable}
\usepackage{booktabs}
\geometry{hmargin={2cm,2cm},vmargin={2.4cm,3cm}}
\usepackage{comment}
\usepackage{listings}
\usepackage{float}
\restylefloat{table}

\usepackage{color}
\definecolor{gray}{rgb}{0.4,0.4,0.4}
\definecolor{darkblue}{rgb}{0.0,0.0,0.6}
\definecolor{cyan}{rgb}{0.0,0.6,0.6}

\setlength{\headheight}{20pt}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Dominik Schreiber, Ji-Ung Lee, Fabian Hirschmann}
\fancyhead[R]{\today}
\fancyfoot[C]{Page \thepage\ of \pageref{LastPage}}

<<setup, include=FALSE>>=
# Install these packages besides the "knitr" package using install.packages("...")
library(xtable)
library(car)

opts_chunk$set(echo=F, breaklines=T)
adt <- read.csv("final_result.csv")
adt$pp <- as.logical(adt$pp)

seldt <- function(classifier, pp) {
    adt[adt$classifier == classifier & adt$pp == T, !names(adt) %in% c("pp", "classifier")]
}
sgd_f <- seldt("SGDClassifier", F)
sgd_t <- seldt("SGDClassifier", T)
nb_f <- seldt("MultinomialNB", F)
nb_t <- seldt("MultinomialNB", T)
@

\begin{document}

\section{Architecture Outline}

The following steps outline the machine learning workflow we have developed:
\begin{itemize}
    \item Preprocessing: We have decided to use a simple space-based tokenization
    (\texttt{tweet.split(' ')}) instead of the one provided by nltk as the nltk
    tokenization produces bad tokens, especially if smileys are involved
    (\texttt{nltk.word\_tokenize(':D :)') -> [':', 'D', ':', ')']}). At the point
    of tokenization it is not clear what is a smiley and what is not, so filtering
    out smiley before tokenizing is no option. As additional preprocessing steps --
    aside of smiley and RT filtering -- we substitute user names with \texttt{<user>},
    hashtags with a more readable version of them (\texttt{\#aVeryLongHashtag -> a very long hashtag})
    and urls with \texttt{<url>} as it might be a valueable information that there
    once was a url/user name. To cope with the huge amount of data we use the
    strong python language feature of \emph{generators}: instead of keeping
    the whole tweet corpus in memory they are lazy-evaluated and loaded on demand.
    \item Vectorization: We have chosen to use the \texttt{HashingVectorizer}
    in lieu of using a \texttt{CountVectorizer} in order to profit from
    the hashing trick. The \texttt{CountVectorizer} holds an in-memory mapping
    from the string tokens to the integer feature indices. The \texttt{HashingVectorizer}
    applies a hash function to the features to determine their column index in the
    sample matrices directly. This results in increased speed and reduced memory
    usage. However, the trade-off here is that it does not provide IDF weighting and
    we opted not to use IDF for decreased training time. In addition, as far
    as tokenization is concerned, we tried fancy token-level analysis (lemmatizing)
    in the tokenization step, but removed this step later on because it did not
    increase our scores.
    Stop words were also removed.
    \item Training: We trained a \texttt{SGDClassifier} and a \texttt{MultinominalNB}
    classifier for with all training samples (\texttt{tweets.all\_shuffled.db}) with minimal
    and full preprocessing. The training is done in a very memory-efficient way due
    to batch processing. Using the Preprocessing Generator (see the first item in this
    list), we iteratively take out a variable yet fixed number of \texttt{BATCH\_SIZE} instances
    and partially train the classifier (using \texttt{partial\_fit}). This allows our
    method to scale to any corpus size, because we don't need to hold the corpus
    in the memory in its entirety.
    \item Evaluation: We evaluated the results using Train/Test-Split with a holdout
    of 5\%. Please see the next section for detailed results.
\end{itemize}
\newpage

\section{Results}
\subsection{With minimal pre-processing}
<<results="asis", fig.height=4.2>>=
plot(f1_score ~ size, data=sgd_f, type="l", log="x", col="blue", xlab="Corpus Size", ylab="F-Score", xlim=c(16,18025887), ylim=c(0,0.7))
lines(f1_score ~ size, data=nb_f, col="red")
grid()
legend("bottomright", c("SGDClassifier", "MultinomialNB"),lty=c(1, 1), lwd=c(2.5, 2.5), col=c("blue", "red"))
@
\subsection{With full pre-processing}
<<results="asis", fig.height=4.2>>=
plot(f1_score ~ size, data=sgd_t, type="l", log="x", col="blue", xlab="Corpus Size", ylab="F-Score", xlim=c(16,18025887), ylim=c(0,0.7))
lines(f1_score ~ size, data=nb_t, col="red")
grid()
legend("bottomright", c("SGDClassifier", "MultinomialNB"),lty=c(1, 1), lwd=c(2.5, 2.5), col=c("blue", "red"))
@
\section{Q: What is better for this project, more data or a better algorithm?}
For this project, the algorithm hardly makes a difference, given that enough
training data is available. However, at some point, no improvement occurs as
far as the scores are concerned. The algorithms we have tried seem to converge
at some point, after which additional training instances serve no purpose.

\appendix
\section{Detailed Results}
<<results="asis">>=
print(xtable(adt), table.placement="H", include.rownames=F, floating=F, tabular.environment="longtable", size="\\footnotesize")
@
In the table above, \texttt{pp} refers to full (\texttt{TRUE}) or partial (\texttt{FALSE}) preprocessing.
\end{document}
