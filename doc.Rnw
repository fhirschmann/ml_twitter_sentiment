\documentclass[twoside]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[ngerman,english]{babel}
\usepackage{fancyhdr,lastpage}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{booktabs}
\geometry{hmargin={2cm,2cm},vmargin={2.4cm,3cm}}
\usepackage{comment}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,matrix}
\usepackage{listings}

\usepackage{color}
\definecolor{gray}{rgb}{0.4,0.4,0.4}
\definecolor{darkblue}{rgb}{0.0,0.0,0.6}
\definecolor{cyan}{rgb}{0.0,0.6,0.6}

\setlength{\headheight}{20pt}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Dominik Schreiber, Ji-Ung Lee, Fabian Hirschmann}
\fancyhead[R]{\today}
\fancyfoot[C]{Page \thepage\ of \pageref{LastPage}}
\begin{document}

\section{Introduction}
\section{Validation and Tuning Results}
\section{Q: What is better for this project, more data or a better algorithm?}
Ideally, both of these points. However, the classifier benefits more from a larger
training corpus. We have tried SVMs and Neural Networks on a smaller corpus and
the results were only slightly better than with Stochastic Gradient Descent or
Ridge Regression whilst the training time increased tremendously. For a larger
corpus and limited resources, it is therefore better to choose a rather fast algorithm.
As far as preprocessing is concerned, it should always be done because it is relatively
cheap to do (in terms of computing time) and the final model benefits greatly from
it.


\end{document}
